{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy\n",
        "\n",
        "**Definition:**  \n",
        "Accuracy is the ratio of correctly predicted observations to the total observations. It measures how often the classifier is correct across all classes. Accuracy provides a general sense of model performance, making it a commonly used metric for evaluating classification models.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Observations}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- **True Positives (TP):** Cases where the model correctly predicts the positive class.\n",
        "- **True Negatives (TN):** Cases where the model correctly predicts the negative class.\n",
        "- **Total Observations:** The total number of instances in the dataset.\n",
        "\n",
        "**Importance of Accuracy:**\n",
        "Accuracy is particularly useful in situations where the class distribution is balanced and there are equal costs for false positives and false negatives. It is widely used in various applications, such as:\n",
        "\n",
        "- **Image Classification:** In identifying objects in images, accuracy can provide a straightforward measure of performance.\n",
        "- **Spam Detection:** In email classification, a high accuracy indicates effective filtering of spam and legitimate emails.\n",
        "\n",
        "**Interpretation:**\n",
        "- **High Accuracy:** A high accuracy value (close to 1) indicates that the model makes correct predictions for the majority of instances.\n",
        "  \n",
        "- **Low Accuracy:** A low accuracy value suggests that the model fails to make correct predictions for a significant portion of the instances, which could be indicative of issues like model overfitting, underfitting, or class imbalance.\n",
        "\n",
        "**Example:**\n",
        "Consider a binary classification problem where we are predicting whether patients have a specific disease. Suppose we have the following results from our predictions:\n",
        "- True Positives (TP): 50 (patients who actually have the disease and were correctly identified)\n",
        "- True Negatives (TN): 30 (patients who do not have the disease and were correctly identified)\n",
        "- False Positives (FP): 10 (patients who do not have the disease but were incorrectly identified as having it)\n",
        "- False Negatives (FN): 10 (patients who have the disease but were incorrectly identified as not having it)\n",
        "\n",
        "The total observations can be calculated as:\n",
        "\n",
        "$$\n",
        "\\text{Total Observations} = TP + TN + FP + FN = 50 + 30 + 10 + 10 = 100\n",
        "$$\n",
        "\n",
        "Now we can calculate the accuracy:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{\\text{Total Observations}} = \\frac{50 + 30}{100} = \\frac{80}{100} = 0.80\n",
        "$$\n",
        "\n",
        "This indicates that the model correctly identifies 80% of all cases.\n",
        "\n",
        "**Relation to Other Metrics:**\n",
        "While accuracy is a useful metric, it is essential to consider it alongside other metrics, especially in cases of class imbalance:\n",
        "- **Precision:** Measures the accuracy of positive predictions.\n",
        "  \n",
        "- **Recall:** Measures the model's ability to identify all positive instances.\n",
        "\n",
        "- **F1 Score:** The harmonic mean of precision and recall, providing a single score that balances both metrics.\n",
        "\n",
        "**Conclusion:**\n",
        "Accuracy is a fundamental metric for evaluating classification models, providing a straightforward assessment of overall performance. However, it is crucial to consider accuracy in conjunction with other metrics, particularly in imbalanced datasets or applications where the cost of false predictions varies. By understanding accuracy alongside precision, recall, and the F1 score, practitioners can gain a comprehensive view of model effectiveness.\n"
      ],
      "metadata": {
        "id": "cdpG4gaBMmsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dox3R-1lMmHN",
        "outputId": "d0c29942-761f-48ef-e91d-2cdc64d0bbad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Labels: [1 0 1 0 0 1 1 0 1 0]\n",
            "True Labels: [1 0 1 1 0 1 0 0 1 0]\n",
            "Accuracy: 0.80\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4 1]\n",
            " [1 4]]\n",
            "\n",
            "True Positives (TP): 4\n",
            "True Negatives (TN): 4\n",
            "False Positives (FP): 1\n",
            "False Negatives (FN): 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
        "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Predicted Labels: {y_pred}\")\n",
        "print(f\"True Labels: {y_true}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "TP = conf_matrix[1, 1]  # True Positives\n",
        "TN = conf_matrix[0, 0]  # True Negatives\n",
        "FP = conf_matrix[0, 1]  # False Positives\n",
        "FN = conf_matrix[1, 0]  # False Negatives\n",
        "\n",
        "print(f\"\\nTrue Positives (TP): {TP}\")\n",
        "print(f\"True Negatives (TN): {TN}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNooZTzSM1ZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}