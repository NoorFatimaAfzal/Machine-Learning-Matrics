{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Precision\n",
        "\n",
        "**Definition:**  \n",
        "Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: *Of all the instances that were predicted as positive, how many were actually positive?* Precision is a key metric in evaluating the performance of classification models, particularly in cases where the cost of false positives is high.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
        "$$\n",
        "\n",
        "**Key Terms:**\n",
        "- **True Positives (TP):** These are cases where the model correctly predicts the positive class. For instance, in a spam detection system, true positives would be emails that are correctly identified as spam.\n",
        "  \n",
        "- **False Positives (FP):** These are cases where the model incorrectly predicts the positive class. Continuing with the spam example, false positives would be legitimate emails that the model mistakenly identifies as spam.\n",
        "  \n",
        "- **False Negatives (FN):** These are cases where the model fails to predict the positive class. In our spam detection example, false negatives would be spam emails that the model incorrectly classifies as not spam.\n",
        "\n",
        "**Importance of Precision:**\n",
        "Precision is particularly crucial in applications where false positives can lead to significant negative consequences. For example:\n",
        "\n",
        "- **Medical Diagnoses:** In a cancer screening test, a false positive could lead to unnecessary biopsies and anxiety for patients.\n",
        "  \n",
        "- **Fraud Detection:** In financial transactions, a false positive may result in blocking legitimate transactions, causing inconvenience for customers.\n",
        "\n",
        "High precision indicates that the model makes reliable positive predictions, which is essential in such sensitive applications.\n",
        "\n",
        "**Interpretation:**\n",
        "- **High Precision:** A high precision value (close to 1) means that when the model predicts a positive class, it is highly likely to be correct. This means the model has a low rate of false positives.\n",
        "  \n",
        "- **Low Precision:** A low precision value indicates that the model generates a significant number of false positives, which can mislead stakeholders and lead to unnecessary actions.\n",
        "\n",
        "**Example:**\n",
        "Let's consider a binary classification problem in a medical testing scenario where the task is to identify patients with a specific disease.\n",
        "\n",
        "Suppose a model predicts 100 patients as having the disease, with the following results:\n",
        "- True Positives (TP): 70 (patients who actually have the disease and were correctly identified)\n",
        "- False Positives (FP): 30 (patients who do not have the disease but were incorrectly identified as having it)\n",
        "\n",
        "Using the precision formula, we calculate precision as follows:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{70}{70 + 30} = \\frac{70}{100} = 0.7\n",
        "$$\n",
        "\n",
        "This indicates that 70% of the patients predicted to have the disease actually do have it. In this case, the precision score is relatively high, but there are still a significant number of false positives that need to be addressed.\n",
        "\n",
        "**Relation to Other Metrics:**\n",
        "Precision is often discussed alongside other important metrics, such as recall and F1 score:\n",
        "\n",
        "- **Recall (Sensitivity):** Recall measures the proportion of actual positives that were correctly identified by the model. It answers the question: *Of all the actual positive instances, how many did we correctly predict?*\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "- **F1 Score:** The F1 score is the harmonic mean of precision and recall, providing a single score that balances both metrics. It is particularly useful in situations with imbalanced datasets, where one class may be significantly more prevalent than the other.\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "**Conclusion:**\n",
        "Precision is a valuable metric for evaluating classification models, especially in cases where the consequences of false positives are significant. Understanding precision helps practitioners make informed decisions about model performance and suitability for specific applications. By considering precision along with recall and other metrics, stakeholders can obtain a more comprehensive view of model performance.\n"
      ],
      "metadata": {
        "id": "c5nHzxVzDyU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmga8YDwB50R",
        "outputId": "110e849e-ed91-4503-aacd-07d116b5a255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Labels: [1 0 1 0 0 1 1 0 1 0]\n",
            "True Labels: [1 0 1 1 0 1 0 0 1 0]\n",
            "Precision: 0.80\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4 1]\n",
            " [1 4]]\n",
            "\n",
            "True Positives (TP): 4\n",
            "False Positives (FP): 1\n",
            "False Negatives (FN): 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, confusion_matrix\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
        "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Predicted Labels: {y_pred}\")\n",
        "print(f\"True Labels: {y_true}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "TP = conf_matrix[1, 1]  # True Positives\n",
        "FP = conf_matrix[0, 1]  # False Positives\n",
        "FN = conf_matrix[1, 0]  # False Negatives\n",
        "\n",
        "print(f\"\\nTrue Positives (TP): {TP}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6eVH3aBFBqB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}