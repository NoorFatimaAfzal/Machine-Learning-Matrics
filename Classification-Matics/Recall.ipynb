{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recall\n",
        "\n",
        "**Definition:**  \n",
        "Recall, also known as sensitivity or true positive rate, is the ratio of correctly predicted positive observations to all actual positives. It answers the question: *Of all the actual positive instances, how many did we correctly predict?* Recall is a crucial metric for evaluating the performance of classification models, especially in contexts where missing a positive instance is costly.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
        "$$\n",
        "\n",
        "**Key Terms:**\n",
        "- **True Positives (TP):** Cases where the model correctly predicts the positive class.\n",
        "- **False Negatives (FN):** Cases where the model fails to predict the positive class (predicted negative but actually positive).\n",
        "\n",
        "**Importance of Recall:**\n",
        "Recall is particularly important in applications where it is critical to identify all positive instances. For example:\n",
        "\n",
        "- **Medical Diagnosis:** In a cancer screening test, a false negative could mean missing a cancer diagnosis, potentially leading to severe health consequences.\n",
        "- **Fraud Detection:** In financial transactions, failing to detect a fraudulent transaction (false negative) could lead to significant financial loss.\n",
        "\n",
        "High recall indicates that the model is effective at identifying positive instances, which is crucial in sensitive applications.\n",
        "\n",
        "**Interpretation:**\n",
        "- **High Recall:** A high recall value (close to 1) means that the model successfully identifies a large proportion of actual positive instances.\n",
        "  \n",
        "- **Low Recall:** A low recall value indicates that the model misses many actual positive instances, which can have serious implications, particularly in critical applications.\n",
        "\n",
        "**Example:**\n",
        "Consider a binary classification problem in a medical testing scenario where the task is to identify patients with a specific disease.\n",
        "\n",
        "Suppose a model predicts 100 patients as having the disease, with the following results:\n",
        "- True Positives (TP): 80 (patients who actually have the disease and were correctly identified)\n",
        "- False Negatives (FN): 20 (patients who have the disease but were incorrectly identified as not having it)\n",
        "\n",
        "Using the recall formula, we calculate recall as follows:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{80}{80 + 20} = \\frac{80}{100} = 0.8\n",
        "$$\n",
        "\n",
        "This indicates that the model correctly identifies 80% of actual positive cases.\n",
        "\n",
        "**Relation to Other Metrics:**\n",
        "Recall is often discussed alongside other important metrics, such as precision and F1 score:\n",
        "- **Precision:** Measures the proportion of predicted positives that are actually positive.\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "- **F1 Score:** The harmonic mean of precision and recall, providing a single score that balances both metrics.\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "**Conclusion:**\n",
        "Recall is a valuable metric for evaluating classification models, especially in cases where failing to identify a positive instance can have serious consequences. Understanding recall helps practitioners make informed decisions about model performance and suitability for specific applications. By considering recall alongside precision and other metrics, stakeholders can obtain a more comprehensive view of model performance.\n"
      ],
      "metadata": {
        "id": "6mJ5pADZF9Mz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFScICrgFjAw",
        "outputId": "179d145c-063e-44c3-fded-af01f82922e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Labels: [1 0 1 0 0 1 1 0 1 0]\n",
            "True Labels: [1 0 1 1 0 1 0 0 1 0]\n",
            "Recall: 0.80\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4 1]\n",
            " [1 4]]\n",
            "\n",
            "True Positives (TP): 4\n",
            "False Negatives (FN): 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import recall_score, confusion_matrix\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
        "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Predicted Labels: {y_pred}\")\n",
        "print(f\"True Labels: {y_true}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "TP = conf_matrix[1, 1]  # True Positives\n",
        "FN = conf_matrix[1, 0]  # False Negatives\n",
        "\n",
        "print(f\"\\nTrue Positives (TP): {TP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JK2h8neaGLuB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}