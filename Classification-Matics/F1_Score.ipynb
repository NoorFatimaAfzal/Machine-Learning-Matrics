{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# F1 Score\n",
        "\n",
        "**Definition:**  \n",
        "The F1 score is the harmonic mean of precision and recall, providing a single score that balances both metrics. It is particularly useful in situations where there is an uneven class distribution, as it helps assess the model's accuracy in identifying positive instances while minimizing false positives and false negatives.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "**Importance of the F1 Score:**\n",
        "The F1 score is an important metric in classification tasks where both precision and recall are crucial. In scenarios where one metric might be favored over the other, the F1 score provides a balanced view of model performance. This is particularly relevant in fields such as:\n",
        "\n",
        "- **Medical Diagnosis:** In scenarios where it is vital to accurately identify patients with a disease (high recall) while ensuring that those identified as positive actually have the disease (high precision).\n",
        "  \n",
        "- **Information Retrieval:** In search engines and recommendation systems, itâ€™s important to return relevant results (high precision) while also retrieving as many relevant documents as possible (high recall).\n",
        "\n",
        "**Interpretation:**\n",
        "- **High F1 Score:** A high F1 score (close to 1) indicates a good balance between precision and recall, suggesting that the model is effective in making accurate predictions with minimal false positives and false negatives.\n",
        "  \n",
        "- **Low F1 Score:** A low F1 score indicates poor model performance, suggesting that either precision, recall, or both are low, leading to a large number of false predictions.\n",
        "\n",
        "**Example:**\n",
        "Consider a binary classification problem where we are predicting whether patients have a particular disease. Suppose we have the following results:\n",
        "- True Positives (TP): 60 (patients who actually have the disease and were correctly identified)\n",
        "- False Positives (FP): 10 (patients who do not have the disease but were incorrectly identified as having it)\n",
        "- False Negatives (FN): 30 (patients who have the disease but were incorrectly identified as not having it)\n",
        "\n",
        "First, we calculate precision and recall:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{60}{60 + 10} = \\frac{60}{70} \\approx 0.857\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{60}{60 + 30} = \\frac{60}{90} \\approx 0.667\n",
        "$$\n",
        "\n",
        "Now we can calculate the F1 score:\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.857 \\times 0.667}{0.857 + 0.667} \\approx 0.75\n",
        "$$\n",
        "\n",
        "This indicates a balanced performance of the model, but there is still room for improvement.\n",
        "\n",
        "**Relation to Other Metrics:**\n",
        "The F1 score is often discussed alongside precision and recall:\n",
        "- **Precision:** Measures the accuracy of positive predictions.\n",
        "  \n",
        "- **Recall:** Measures the model's ability to identify all positive instances.\n",
        "\n",
        "**Conclusion:**\n",
        "The F1 score is a crucial metric for evaluating classification models, especially in cases where precision and recall are equally important. Understanding the F1 score helps practitioners make informed decisions about model performance and suitability for specific applications. By considering the F1 score along with precision and recall, stakeholders can obtain a more comprehensive view of model effectiveness."
      ],
      "metadata": {
        "id": "hbaFnNuDHyQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
        "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Predicted Labels: {y_pred}\")\n",
        "print(f\"True Labels: {y_true}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "TP = conf_matrix[1, 1]  # True Positives\n",
        "FP = conf_matrix[0, 1]  # False Positives\n",
        "FN = conf_matrix[1, 0]  # False Negatives\n",
        "\n",
        "print(f\"\\nTrue Positives (TP): {TP}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_smlreGHwFu",
        "outputId": "3476b6ba-a24d-4642-bc32-8a36d8e102c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Labels: [1 0 1 0 0 1 1 0 1 0]\n",
            "True Labels: [1 0 1 1 0 1 0 0 1 0]\n",
            "Precision: 0.80\n",
            "Recall: 0.80\n",
            "F1 Score: 0.80\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4 1]\n",
            " [1 4]]\n",
            "\n",
            "True Positives (TP): 4\n",
            "False Positives (FP): 1\n",
            "False Negatives (FN): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-430l268K0KM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}